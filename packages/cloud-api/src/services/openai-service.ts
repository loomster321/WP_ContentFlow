import OpenAI from 'openai';
import { AIGenerationResponse } from '@wp-content-flow/shared-types';
import { aiLogger } from '../utils/logger';

interface OpenAIRequest {
  model: string;
  messages: Array<{ role: 'system' | 'user' | 'assistant'; content: string }>;
  temperature: number;
  maxTokens: number;
}

export class OpenAIService {
  private client: OpenAI;

  constructor() {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      throw new Error('OPENAI_API_KEY environment variable is required');
    }

    this.client = new OpenAI({
      apiKey: apiKey
    });
  }

  async generateContent(request: OpenAIRequest): Promise<AIGenerationResponse> {
    const startTime = Date.now();
    
    try {
      aiLogger.info('Sending request to OpenAI', {
        model: request.model,
        messageCount: request.messages.length,
        temperature: request.temperature,
        maxTokens: request.maxTokens
      });

      const completion = await this.client.chat.completions.create({
        model: request.model,
        messages: request.messages,
        temperature: request.temperature,
        max_tokens: request.maxTokens,
        stream: false
      });

      const content = completion.choices[0]?.message?.content || '';
      const usage = completion.usage;
      
      if (!content) {
        throw new Error('No content generated by OpenAI');
      }

      const processingTime = (Date.now() - startTime) / 1000;

      // Calculate confidence score based on response quality indicators
      const confidenceScore = this.calculateConfidenceScore(content, completion);

      const response: AIGenerationResponse = {
        content,
        confidenceScore,
        tokenUsage: {
          promptTokens: usage?.prompt_tokens || 0,
          completionTokens: usage?.completion_tokens || 0,
          totalTokens: usage?.total_tokens || 0
        },
        processingTime,
        agentUsed: 'openai',
        metadata: {
          model: request.model,
          temperature: request.temperature,
          finishReason: completion.choices[0]?.finish_reason,
          openaiId: completion.id
        }
      };

      aiLogger.info('OpenAI request completed', {
        contentLength: content.length,
        tokenUsage: response.tokenUsage,
        processingTime,
        confidenceScore
      });

      return response;

    } catch (error) {
      const processingTime = (Date.now() - startTime) / 1000;
      
      aiLogger.error('OpenAI request failed', {
        error: error instanceof Error ? error.message : String(error),
        processingTime,
        model: request.model
      });

      throw error;
    }
  }

  /**
   * Calculate confidence score based on response characteristics
   */
  private calculateConfidenceScore(content: string, completion: OpenAI.Chat.Completions.ChatCompletion): number {
    let score = 0.5; // Base score

    // Content length indicator (not too short, not cut off)
    const wordCount = content.split(' ').length;
    if (wordCount >= 10 && wordCount <= 1000) {
      score += 0.2;
    }

    // Completion finished naturally
    if (completion.choices[0]?.finish_reason === 'stop') {
      score += 0.2;
    }

    // Content quality indicators
    const hasStructure = content.includes('.') || content.includes('!') || content.includes('?');
    if (hasStructure) {
      score += 0.1;
    }

    // Not cut off abruptly
    const endsWell = content.trim().match(/[.!?]$/);
    if (endsWell) {
      score += 0.1;
    }

    return Math.min(score, 1.0);
  }

  /**
   * Test API connection and configuration
   */
  async testConnection(): Promise<boolean> {
    try {
      await this.client.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: [{ role: 'user', content: 'Test connection' }],
        max_tokens: 5,
        temperature: 0.1
      });
      
      aiLogger.info('OpenAI connection test successful');
      return true;
    } catch (error) {
      aiLogger.error('OpenAI connection test failed', {
        error: error instanceof Error ? error.message : String(error)
      });
      return false;
    }
  }

  /**
   * Get available models
   */
  async getAvailableModels(): Promise<string[]> {
    try {
      const models = await this.client.models.list();
      const chatModels = models.data
        .filter(model => model.id.includes('gpt'))
        .map(model => model.id)
        .sort();
      
      return chatModels;
    } catch (error) {
      aiLogger.error('Failed to fetch OpenAI models', {
        error: error instanceof Error ? error.message : String(error)
      });
      return ['gpt-3.5-turbo', 'gpt-4', 'gpt-4-turbo-preview']; // Fallback
    }
  }
}